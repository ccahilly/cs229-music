{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f3456188",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from google.cloud import storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "50c723a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c345484",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    }
   ],
   "source": [
    "# class AudioToTextSmallModel(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(AudioToTextSmallModel, self).__init__()\n",
    "#         # Initialize T5 model and tokenizer\n",
    "#         self.t5 = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
    "\n",
    "#     def forward(self, audio_embeddings, labels=None):\n",
    "#         # Ensure correct shape for inputs_embeds: (batch_size, seq_length, embedding_dim)\n",
    "#         # T5 expects the shape (batch_size, seq_length, embedding_dim)\n",
    "#         projected_embeddings = audio_embeddings.unsqueeze(1)  # Add seq_length dimension (usually 1 for this case)\n",
    "\n",
    "#         # Generate outputs with T5\n",
    "#         outputs = self.t5(\n",
    "#             inputs_embeds=projected_embeddings,\n",
    "#             labels=labels\n",
    "#         )\n",
    "#         return outputs\n",
    "    \n",
    "class AudioToTextBaseModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AudioToTextBaseModel, self).__init__()\n",
    "        # Initialize T5 model and tokenizer with t5-large\n",
    "        self.t5 = T5ForConditionalGeneration.from_pretrained(\"t5-base\")\n",
    "        # Linear layer to project 512-dimensional CLAP embeddings to 1024-dimensional embeddings\n",
    "        self.projection_layer = nn.Linear(512, 768)\n",
    "\n",
    "    def forward(self, audio_embeddings, labels=None):\n",
    "        # Project audio embeddings from 512 to 1024 dimensions\n",
    "        projected_embeddings = self.projection_layer(audio_embeddings)\n",
    "        \n",
    "        # Add seq_length dimension (usually 1 for this case)\n",
    "        projected_embeddings = projected_embeddings.unsqueeze(1)\n",
    "\n",
    "        # Generate outputs with T5\n",
    "        outputs = self.t5(\n",
    "            inputs_embeds=projected_embeddings,\n",
    "            labels=labels\n",
    "        )\n",
    "        return outputs\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-base\")\n",
    "    \n",
    "# Initialize the model and tokenizer\n",
    "model = AudioToTextBaseModel().to(device)  # Move the model to GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "105e3869",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/tmp/ipykernel_159011/1859719088.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"../data/weights_10_base.pth\"))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AudioToTextBaseModel(\n",
       "  (t5): T5ForConditionalGeneration(\n",
       "    (shared): Embedding(32128, 768)\n",
       "    (encoder): T5Stack(\n",
       "      (embed_tokens): Embedding(32128, 768)\n",
       "      (block): ModuleList(\n",
       "        (0): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (relative_attention_bias): Embedding(32, 12)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseActDense(\n",
       "                (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "                (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (act): ReLU()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1-11): 11 x T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseActDense(\n",
       "                (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "                (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (act): ReLU()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (final_layer_norm): T5LayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (decoder): T5Stack(\n",
       "      (embed_tokens): Embedding(32128, 768)\n",
       "      (block): ModuleList(\n",
       "        (0): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (relative_attention_bias): Embedding(32, 12)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerCrossAttention(\n",
       "              (EncDecAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (2): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseActDense(\n",
       "                (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "                (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (act): ReLU()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1-11): 11 x T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerCrossAttention(\n",
       "              (EncDecAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (2): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseActDense(\n",
       "                (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "                (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (act): ReLU()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (final_layer_norm): T5LayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (lm_head): Linear(in_features=768, out_features=32128, bias=False)\n",
       "  )\n",
       "  (projection_layer): Linear(in_features=512, out_features=768, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(\"../data/weights_10_base.pth\"))\n",
    "\n",
    "# Step 4: Set the model to evaluation mode (if you only need to do inference)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "19fd3d41",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/tmp/ipykernel_159011/4004037917.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  train_data = torch.load('../data/train_data.pt')\n",
      "/var/tmp/ipykernel_159011/4004037917.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  test_data = torch.load('../data/test_data.pt')\n"
     ]
    }
   ],
   "source": [
    "# Load the training data\n",
    "train_data = torch.load('../data/train_data.pt')\n",
    "test_data = torch.load('../data/test_data.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dd8c05ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_embeddings = torch.tensor(np.array(train_data[\"embeddings\"])).to(device)  # Move to GPU\n",
    "train_labels = [str(label) for label in train_data[\"labels\"]]\n",
    "\n",
    "test_embeddings = torch.tensor(np.array(test_data[\"embeddings\"])).to(device)  # Move to GPU\n",
    "test_labels = [str(label) for label in test_data[\"labels\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e683d8b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3553, 512])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_embeddings.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "289bde36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def average_pairwise_sim(embeddings):\n",
    "    pairwise_similarities = cosine_similarity(embeddings)\n",
    "\n",
    "    # Get the number of embeddings\n",
    "    num_embeddings = len(embeddings)\n",
    "\n",
    "    # Extract the upper triangle of the similarity matrix without the diagonal\n",
    "    upper_triangle_indices = np.triu_indices(num_embeddings, k=1)\n",
    "    pairwise_values = pairwise_similarities[upper_triangle_indices]\n",
    "\n",
    "    # Calculate the average pairwise similarity\n",
    "    average_pairwise_similarity = np.mean(pairwise_values)\n",
    "    return average_pairwise_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1bc8ce58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.93777186"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "average_pairwise_sim(train_embeddings.cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8c0cde13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 7.9709e-03,  1.4819e-02,  1.5407e-02, -3.9049e-03, -9.9480e-03,\n",
       "        -6.9047e-03,  3.6069e-02, -1.0557e-02,  1.5398e-02, -7.1141e-03,\n",
       "        -5.6794e-04,  1.3234e-02, -2.1287e-02,  3.0202e-02,  4.8684e-03,\n",
       "        -2.1075e-02, -1.1806e-02, -1.3603e-02, -8.3285e-03, -2.3117e-03,\n",
       "         6.5029e-03, -3.1472e-02, -5.3021e-03, -6.0198e-03,  7.7293e-03,\n",
       "         6.9554e-02, -2.4552e-02, -1.1826e-02, -3.4772e-02, -1.9395e-02,\n",
       "        -1.3569e-02,  2.3409e-02,  4.8719e-02,  6.3505e-02, -4.9649e-02,\n",
       "         1.0479e-02,  9.4825e-03, -7.4857e-03, -7.8678e-03, -1.1279e-02,\n",
       "        -1.7770e-02,  1.6087e-02,  2.3782e-02, -2.3731e-02,  1.6549e-02,\n",
       "        -6.6485e-03,  1.0770e-02,  5.4051e-02,  5.1967e-04,  2.1856e-03,\n",
       "         3.4155e-02, -1.7571e-02, -2.3475e-02, -3.0246e-03,  6.2090e-02,\n",
       "        -5.8932e-03, -5.5003e-02, -1.3057e-02, -2.1058e-02, -1.6350e-02,\n",
       "        -4.8270e-03,  1.7186e-02,  4.0746e-02, -7.7797e-04,  3.9195e-02,\n",
       "        -9.1315e-03, -4.7628e-03,  3.5974e-02, -3.2275e-03,  1.6180e-02,\n",
       "         3.9288e-02, -3.7728e-02, -8.5115e-03,  2.8715e-02, -2.7476e-02,\n",
       "         5.9529e-03,  8.2782e-03, -9.1686e-04, -4.4042e-03,  4.1087e-02,\n",
       "         4.8877e-02,  2.1234e-02,  4.7678e-02, -3.2845e-02,  2.6229e-02,\n",
       "        -3.9373e-02, -3.5306e-03, -1.7607e-02, -1.3354e-02,  2.2902e-02,\n",
       "        -1.4098e-02,  2.5255e-02, -1.7710e-02, -3.6115e-02,  3.1851e-03,\n",
       "        -1.2354e-02,  1.8076e-02,  4.5288e-03,  7.8391e-03,  4.3918e-03,\n",
       "        -1.6197e-03,  2.1758e-03, -5.3630e-02, -6.4710e-03, -1.1397e-02,\n",
       "        -2.9623e-03, -1.8222e-02,  3.2338e-02,  5.6700e-03, -2.8175e-02,\n",
       "         1.6115e-02,  3.8039e-02, -1.2452e-02,  8.6535e-03, -1.2277e-02,\n",
       "         1.3732e-02, -8.6684e-03,  5.1606e-02, -1.3444e-02, -4.5508e-02,\n",
       "         1.0627e-02,  2.3601e-02, -4.0345e-02, -4.0614e-02, -4.0944e-03,\n",
       "        -5.1017e-02, -7.1327e-03,  1.2111e-02, -4.5926e-03, -3.0477e-02,\n",
       "        -2.1146e-02,  1.2019e-02, -6.7720e-03,  7.2615e-03,  2.2156e-02,\n",
       "         2.4008e-02, -5.7243e-02, -3.2044e-02,  1.4758e-02, -1.1827e-03,\n",
       "         1.3451e-02,  2.0353e-02, -4.2894e-03,  1.6338e-02, -6.4660e-02,\n",
       "         1.1944e-02,  5.2987e-03, -1.9585e-02, -6.2786e-03,  8.7136e-03,\n",
       "         1.2926e-02, -5.1169e-02,  4.0619e-03, -9.3519e-03,  4.6230e-02,\n",
       "        -2.6214e-02,  1.0843e-02, -1.6879e-02,  1.2405e-02, -2.0614e-02,\n",
       "         5.5244e-02,  5.5145e-05, -1.7209e-02,  1.0467e-02, -2.5549e-02,\n",
       "         1.7824e-02,  3.3662e-02,  2.2933e-02,  6.7680e-03, -2.2528e-02,\n",
       "         2.0610e-02, -6.0439e-03, -1.2501e-02, -1.7437e-02,  1.4779e-02,\n",
       "        -2.8067e-02,  1.0504e-03, -3.3102e-02, -1.7897e-02,  1.7903e-02,\n",
       "        -2.1469e-02,  7.3407e-03, -9.4077e-03, -5.5249e-03, -1.6681e-02,\n",
       "         7.3451e-03, -4.5685e-03, -4.7747e-02, -1.8902e-02,  1.8068e-02,\n",
       "         4.4359e-02, -1.7991e-02, -4.0420e-02, -2.8065e-02, -1.3762e-02,\n",
       "         8.5188e-03,  3.6548e-02, -7.2352e-03, -2.4338e-02, -8.8387e-03,\n",
       "        -3.1767e-04,  2.9480e-02,  1.2067e-03,  1.1853e-02,  8.5488e-04,\n",
       "        -2.2107e-02, -2.8548e-02, -1.4704e-02, -3.0221e-02, -3.1247e-02,\n",
       "        -3.0799e-02,  1.5514e-04, -1.5628e-02, -2.5260e-02, -7.8731e-03,\n",
       "        -1.3143e-02,  3.8639e-02,  3.9073e-02, -2.1129e-02, -1.0731e-02,\n",
       "         4.8130e-03,  5.3828e-03,  1.8797e-02, -2.9968e-02, -1.5174e-02,\n",
       "         2.0679e-02,  3.9967e-02,  1.4766e-02,  2.9090e-02, -5.9672e-02,\n",
       "        -1.5996e-02, -2.1508e-02, -6.1538e-03, -1.2575e-03, -2.0690e-02,\n",
       "        -6.3821e-03,  2.8619e-02,  1.8873e-02, -2.3819e-03, -4.5210e-03,\n",
       "        -3.0423e-02, -1.2444e-02, -1.9466e-02,  3.0063e-03, -4.0897e-02,\n",
       "        -1.5137e-02, -8.0411e-03,  1.0067e-02,  6.1483e-02, -1.5849e-02,\n",
       "         1.9465e-02, -3.2070e-02, -1.0759e-02, -1.9509e-02, -1.2120e-02,\n",
       "         3.4021e-03, -2.5686e-02, -2.8111e-02,  2.0848e-02, -6.8181e-03,\n",
       "        -1.9212e-02, -4.0413e-02, -9.2531e-03, -1.4689e-02, -1.8964e-02,\n",
       "        -2.8907e-02,  1.5705e-02,  1.3784e-02,  2.9348e-02, -2.4005e-02,\n",
       "         3.4788e-03,  2.7024e-02, -3.0306e-02, -1.1191e-02, -1.3985e-02,\n",
       "         1.5639e-02, -2.7651e-02, -1.2329e-02, -2.4061e-02,  6.3756e-02,\n",
       "         3.2294e-02,  1.4633e-02,  8.3574e-03,  4.0536e-04, -3.3734e-02,\n",
       "         1.8812e-02,  1.6384e-02,  1.4613e-02,  1.8286e-02, -2.8370e-02,\n",
       "         2.2060e-02,  5.4465e-03, -5.3879e-02,  4.9701e-02,  2.7438e-03,\n",
       "        -3.3189e-02, -3.6493e-02, -2.9667e-02,  2.5846e-02, -2.8341e-02,\n",
       "        -2.0547e-02,  2.1867e-02,  6.7268e-03,  1.9398e-02, -2.1680e-02,\n",
       "        -4.8547e-02,  5.1775e-02,  3.0846e-02, -8.4437e-04, -1.3884e-03,\n",
       "         1.1082e-02,  1.3924e-02,  1.9995e-02, -4.1311e-02, -1.7383e-02,\n",
       "         3.5480e-03,  8.0859e-03,  1.7404e-02, -6.1875e-02,  2.1568e-02,\n",
       "        -1.8840e-02, -4.2152e-02, -6.8918e-03,  1.2386e-02,  2.0278e-02,\n",
       "         9.9125e-03,  4.3916e-03,  3.4363e-02,  9.0808e-03,  1.1717e-02,\n",
       "         2.0456e-02,  2.4737e-02, -1.2907e-02, -1.2224e-02, -4.2185e-02,\n",
       "        -7.6633e-02,  2.2743e-02,  1.3024e-02,  2.3899e-02,  1.2071e-02,\n",
       "         8.2042e-03,  3.6584e-02, -1.4051e-02, -3.0695e-03, -9.6402e-03,\n",
       "        -1.9833e-03, -4.2033e-02, -5.9467e-02,  3.2794e-02,  1.8006e-02,\n",
       "         1.7455e-02, -5.7583e-03, -2.0520e-04,  1.9469e-02,  2.1737e-03,\n",
       "        -2.2264e-02, -2.0752e-02,  6.2289e-03,  7.1811e-03,  4.2435e-02,\n",
       "         9.6537e-04, -1.6143e-02,  1.4716e-02,  1.9133e-02,  1.5813e-02,\n",
       "        -5.8977e-02, -3.4702e-02, -2.5020e-02,  9.0713e-03,  3.3074e-03,\n",
       "        -1.7362e-02, -2.5601e-02,  2.6268e-03, -3.3892e-03,  1.3001e-02,\n",
       "         4.5864e-03, -1.0532e-02,  2.5660e-02, -6.9039e-03,  3.6472e-02,\n",
       "        -1.7824e-03, -2.6662e-03, -3.9047e-02, -2.3158e-03, -1.9777e-02,\n",
       "         9.0157e-03, -4.4937e-02, -4.0296e-02, -1.0861e-02, -4.4994e-02,\n",
       "         6.3696e-03, -2.3870e-02, -1.6297e-03, -6.5089e-03, -5.4938e-03,\n",
       "        -2.9274e-02, -1.6967e-02,  4.7843e-02, -1.4336e-02,  3.0615e-02,\n",
       "        -2.2034e-02,  1.4693e-03, -4.5488e-03,  1.1960e-02, -3.2796e-02,\n",
       "         1.2219e-02, -1.7270e-02,  2.9872e-03,  4.1125e-02, -4.7000e-02,\n",
       "         8.5494e-03, -2.9685e-03,  2.8882e-02, -3.1747e-03,  4.5187e-02,\n",
       "        -5.0067e-02,  8.1282e-03, -5.1982e-02, -2.0970e-03, -2.2905e-02,\n",
       "         3.1546e-02,  4.2708e-03, -3.6075e-02, -3.2037e-04,  2.8104e-02,\n",
       "         2.2816e-02, -1.3321e-02,  3.3976e-03,  1.4063e-02,  5.4858e-04,\n",
       "         3.7873e-03, -8.1834e-03, -1.0439e-02, -1.5885e-02, -1.3658e-02,\n",
       "        -9.9501e-03,  8.2680e-03,  4.5571e-02, -1.7864e-02,  2.5942e-02,\n",
       "        -6.4807e-03, -4.3140e-02, -1.7817e-02, -2.0280e-02,  3.7507e-02,\n",
       "         2.3889e-02,  2.3888e-02,  2.9239e-03,  7.9336e-03,  3.4924e-03,\n",
       "         1.7247e-02,  7.2203e-03, -7.2462e-03,  1.8627e-02,  2.9765e-02,\n",
       "        -3.0487e-03, -2.2952e-02, -1.5992e-03,  2.1260e-02,  5.7024e-03,\n",
       "        -1.4057e-02, -6.2031e-02,  3.0062e-03, -1.1646e-02,  3.9649e-02,\n",
       "        -1.5776e-02, -2.2683e-02, -1.0137e-02, -5.7077e-03, -1.1387e-02,\n",
       "         2.0049e-03, -1.3692e-02, -4.6339e-03, -3.2148e-02,  1.2596e-02,\n",
       "        -4.4326e-03, -3.0885e-02,  1.0193e-02,  2.0416e-02, -2.2506e-02,\n",
       "        -3.8384e-03,  6.1425e-02, -6.2816e-03, -2.0655e-02, -3.8130e-03,\n",
       "         2.2487e-02,  7.3288e-03, -6.0956e-03, -5.1767e-03,  1.5250e-02,\n",
       "         5.5907e-03, -4.5291e-02,  8.2494e-03,  7.6211e-03,  1.2422e-02,\n",
       "        -2.4917e-02,  3.6378e-02, -3.8647e-03,  2.6578e-02, -4.1566e-02,\n",
       "        -9.3416e-03, -1.0646e-02,  9.4148e-04,  5.7858e-04, -6.1543e-02,\n",
       "        -3.4217e-02, -1.3871e-02, -1.3793e-03,  2.1182e-02, -1.3518e-02,\n",
       "         3.0573e-02,  6.8082e-03], device='cuda:0')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_embeddings[0] - train_embeddings[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "18fcec46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.93470097"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "average_pairwise_sim(test_embeddings.cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a1f6508e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure all labels are strings\n",
    "for label in train_labels:\n",
    "    if label is None or not isinstance(label, str):\n",
    "        print(\"Label has an error or is not a string\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "15e4d8af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the labels (convert them into token IDs) just once\n",
    "train_tokenized_labels = tokenizer(train_labels, padding=True, truncation=True, return_tensors=\"pt\").input_ids.to(device)  # Move to GPU\n",
    "\n",
    "test_tokenized_labels = tokenizer(test_labels, padding=True, truncation=True, return_tensors=\"pt\").input_ids.to(device)  # Move to GPU\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6c7e2216",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataLoader for your train data\n",
    "train_dataset = TensorDataset(train_embeddings, train_tokenized_labels)\n",
    "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "test_dataset = TensorDataset(test_embeddings, test_tokenized_labels)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "95804ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.AdamW(model.parameters(), lr=1e-5)  # You can adjust the learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4f0b8c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_final_loss(model, data_loader):\n",
    "    total_loss = 0\n",
    "    for i, batch in enumerate(data_loader):\n",
    "        audio_embeddings, labels = batch\n",
    "\n",
    "        # Move data to GPU\n",
    "        audio_embeddings = audio_embeddings.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(audio_embeddings, labels=labels)\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    # Calculate and print the loss for this epoch\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    return avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9fb8b183",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9146866494589982"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_final_loss(model, train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4a8f327e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0046113055013608"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_final_loss(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a472a29b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AudioToTextBaseModel(\n",
       "  (t5): T5ForConditionalGeneration(\n",
       "    (shared): Embedding(32128, 768)\n",
       "    (encoder): T5Stack(\n",
       "      (embed_tokens): Embedding(32128, 768)\n",
       "      (block): ModuleList(\n",
       "        (0): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (relative_attention_bias): Embedding(32, 12)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseActDense(\n",
       "                (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "                (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (act): ReLU()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1-11): 11 x T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseActDense(\n",
       "                (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "                (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (act): ReLU()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (final_layer_norm): T5LayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (decoder): T5Stack(\n",
       "      (embed_tokens): Embedding(32128, 768)\n",
       "      (block): ModuleList(\n",
       "        (0): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (relative_attention_bias): Embedding(32, 12)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerCrossAttention(\n",
       "              (EncDecAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (2): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseActDense(\n",
       "                (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "                (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (act): ReLU()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1-11): 11 x T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerCrossAttention(\n",
       "              (EncDecAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (2): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseActDense(\n",
       "                (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "                (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (act): ReLU()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (final_layer_norm): T5LayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (lm_head): Linear(in_features=768, out_features=32128, bias=False)\n",
       "  )\n",
       "  (projection_layer): Linear(in_features=512, out_features=768, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9c81cef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(example_embedding):\n",
    "    with torch.no_grad():\n",
    "        input_embeddings = model.projection_layer(example_embedding)\n",
    "        generated_ids = model.t5.generate(\n",
    "            inputs_embeds=input_embeddings.view(1, 1, 768),\n",
    "            max_length=100,  # Adjust as needed\n",
    "            early_stopping=True\n",
    "        )\n",
    "    return tokenizer.decode(generated_ids[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6c591d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def run_predictions(model, data_loader):\n",
    "    pred_text = []\n",
    "    true_text = []\n",
    "    for i, batch in tqdm(enumerate(data_loader)):\n",
    "        if i == 50:\n",
    "            break\n",
    "        audio_embeddings, labels = batch\n",
    "        pred = inference(audio_embeddings[0])\n",
    "        true = tokenizer.decode(labels[0], skip_special_tokens=True)\n",
    "        pred_text.append(pred)\n",
    "        true_text.append(true)\n",
    "\n",
    "    # Calculate and print the loss for this epoch\n",
    "    return true_text, pred_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "270779b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:638: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n",
      "  warnings.warn(\n",
      "50it [01:18,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The low quality recording features a latin jazz song played in the background over which a drums solo is played. The solo consists of shimmering hi hats, punchy snare and kick hits and low tom rolls, while the latin jazz song consists of groovy piano chords and wooden percussion. It sounds energetic and exciting.\n",
      "This is a live performance of a classical music piece. The tempo is medium with a groovy bass line, groovy bass line, groovy bass line, groovy bass line, groovy bass line, groovy bassline, groovy bassline, groovy bassline, groovy bassline, groovy bass\n",
      "This is an amateur recording of a dance performance. There is a zumba dance music version of a movie theme playing in the background. The melody is being played by the strings and the keyboard while there is a loud electronic drum beat for the rhythm. There is a mysterious yet energetic feel to this piece. The recording quality is not that great. However, this piece could still be used to gather samples for beat-making.\n",
      "This is a live performance of a classical music piece. The tempo is medium with a groovy bass line, groovy bass line, groovy bass line, groovy bassline, groovy bassline, groovy bassline, groovy bassline, groovy bassline, groovy bassline, groovy bass\n",
      "The low quality recording features a classical song that contains a wide string melody. It sounds suspenseful and intense.\n",
      "This is a live performance of a classical music piece. The tempo is medium with a groovy bass line, groovy bass line, groovy bass line, groovy bass line, groovy bassline, groovy bassline, groovy bassline, groovy bassline, groovy bassline, groovy bass\n",
      "This audio contains someone playing a classical piece on a cembalo. This song may be playing live while listening to a concert.\n",
      "This is a live performance of a classical music piece. The tempo is medium with a groovy bass line, groovy bass line, groovy bass line, groovy bass line, groovy bassline, groovy bassline, groovy bassline, groovy bassline, groovy bassline, groovy bass\n",
      "This is a high-octane blues song, with layers of intricate bluesy acoustic guitar playing. There's a slide guitar involved, and other layers of potent intense guitar playing. There's a distant filtered vocal hum which sounds like it is coming through a telephone.\n",
      "This is a live performance of a classical music piece. The tempo is medium with a groovy bass line, groovy bass line, groovy bass line, groovy bass line, groovy bassline, groovy bassline, groovy bassline, groovy bassline, groovy bassline, groovy bass\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_true, train_pred = run_predictions(model, train_loader)\n",
    "for i in range(5):\n",
    "    print(train_true[i])\n",
    "    print(train_pred[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "29f5c6a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "50it [01:17,  1.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This house music features a female voice singing the main melody. This is accompanied by programmed percussion playing a simple beat. The kick is played on every count. Hand claps are played at every alternate count. The bass plays the root notes of the chords. Synth chords are played in the background. This song can be played at a club.\n",
      "This is a live performance of a jazz song. The tempo is medium with a groovy bass line, groovy bass line, groovy bass line, groovy bass line, groovy bass line, groovy bass line, groovy bass line, groovy bass line, groovy bass line, groovy bass line\n",
      "This is instrumental Chinese music. The main melody is played by dizi, a Chinese flute. Qin (a Chinese xylophone) is played with syncopation while at the same time carrying the bass notes. There is also a simple acoustic drum beat in the rhythmic background. The piece has a positive, optimistic atmosphere. It could be used in the movies/shows that take place in China. It could also be used in the background of a Chinese cuisine restaurant.\n",
      "This is a live performance of a classical music piece. The tempo is medium with a groovy bass line, groovy bass line, groovy bass line, groovy bass line, groovy bass line, groovy bass line, groovy bass line, groovy bass line, groovy bass line, groovy bass\n",
      "The low quality recording features a pop song that consists of passionate male vocal singing over punchy \"4 on the floor\" kick pattern, groovy accordion melody, wide tinny hi hats, claps and groovy bass. In the second half of the loop, the instrumental consists of claps, \"4 on the floor\" kick pattern without the bass, soft crash cymbal, addictive synth melody and wide chicken clucking sound effect. There is a sweep riser at the very end of the loop. It sounds fun and happy, like something kids would listen to.\n",
      "This is a live performance of a classical music piece. The tempo is medium with a groovy bass line, groovy bass line, groovy bass line, groovy bass line, groovy bass line, groovy bassline, groovy bassline, groovy bassline, groovy bassline, groovy bass\n",
      "Someone is playing acoustic guitar. While a higher pitched male voice is singing a sad song. This is an amateur recording. This song may be playing at home on a rainy day.\n",
      "This is a live performance of a classical music piece. The tempo is medium with a groovy bass line, groovy bass line, groovy bass line, groovy bass line, groovy bass line, groovy bassline, groovy bassline, groovy bassline, groovy bassline, groovy bass\n",
      "This song features a synth lead in the beginning. This is accompanied by Indian percussion. A synth choir is playing chords in the background. The synth lead pauses along with the other instruments. A techno beat is played using only bass and kick. This song can be played at a DJ party.\n",
      "This is a live performance of a classical music piece. The tempo is medium with a groovy bass line, groovy bass line, groovy bass line, groovy bass line, groovy bass line, groovy bassline, groovy bassline, groovy bassline, groovy bassline, groovy bass\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test_true, test_pred = run_predictions(model, test_loader)\n",
    "for i in range(5):\n",
    "    print(test_true[i])\n",
    "    print(test_pred[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "fefc3919",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This house music features a female voice singing the main melody. This is accompanied by programmed percussion playing a simple beat. The kick is played on every count. Hand claps are played at every alternate count. The bass plays the root notes of the chords. Synth chords are played in the background. This song can be played at a club.\n",
      "This is a live performance of a jazz song. The tempo is medium with a groovy bass line, groovy bass line, groovy bass line, groovy bass line, groovy bass line, groovy bass line, groovy bass line, groovy bass line, groovy bass line, groovy bass line\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "test_idx = random.randint(0,1000)\n",
    "print(test_true[test_idx])\n",
    "print(test_pred[test_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3fc6226d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "bert_model = SentenceTransformer('bert-base-nli-mean-tokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f07d113e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "\n",
    "def evaluate_scores(true, pred):\n",
    "    # Split the strings into tokens\n",
    "    scores = {}\n",
    "    scores[\"bleu\"] = []\n",
    "    scores[\"bert_sim\"] = []\n",
    "    \n",
    "    for i in range(len(true)):\n",
    "        reference = true[i].split()\n",
    "        candidate = pred[i].split()\n",
    "\n",
    "        # Calculate the BLEU score\n",
    "        bleu_score = sentence_bleu([reference], candidate, weights=(0.25, 0.25, 0.25, 0.25))\n",
    "        scores[\"bleu\"].append(bleu_score)\n",
    "        \n",
    "        # Initializing the Sentence Transformer model using BERT with mean-tokens pooling\n",
    "        \n",
    "\n",
    "        # Encoding the sentences to obtain their embeddings\n",
    "        sentence_embeddings = bert_model.encode([true[i], pred[i]])\n",
    "\n",
    "        # Calculating the cosine similarity between the first sentence embedding and the rest of the embeddings\n",
    "        # The result will be a list of similarity scores between the first sentence and each of the other sentences\n",
    "        similarity_score = cosine_similarity([sentence_embeddings[0]], [sentence_embeddings[1]])[0][0]\n",
    "        \n",
    "        scores[\"bert_sim\"].append(similarity_score)\n",
    "    return scores\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7c0c9db8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/opt/conda/lib/python3.10/site-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/opt/conda/lib/python3.10/site-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/opt/conda/lib/python3.10/site-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/opt/conda/lib/python3.10/site-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/opt/conda/lib/python3.10/site-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/opt/conda/lib/python3.10/site-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/opt/conda/lib/python3.10/site-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/opt/conda/lib/python3.10/site-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/opt/conda/lib/python3.10/site-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/opt/conda/lib/python3.10/site-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/opt/conda/lib/python3.10/site-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/opt/conda/lib/python3.10/site-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/opt/conda/lib/python3.10/site-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/opt/conda/lib/python3.10/site-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/opt/conda/lib/python3.10/site-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/opt/conda/lib/python3.10/site-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/opt/conda/lib/python3.10/site-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/opt/conda/lib/python3.10/site-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/opt/conda/lib/python3.10/site-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/opt/conda/lib/python3.10/site-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/opt/conda/lib/python3.10/site-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/opt/conda/lib/python3.10/site-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/opt/conda/lib/python3.10/site-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/opt/conda/lib/python3.10/site-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/opt/conda/lib/python3.10/site-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/opt/conda/lib/python3.10/site-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/opt/conda/lib/python3.10/site-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/opt/conda/lib/python3.10/site-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/opt/conda/lib/python3.10/site-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/opt/conda/lib/python3.10/site-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/opt/conda/lib/python3.10/site-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/opt/conda/lib/python3.10/site-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/opt/conda/lib/python3.10/site-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/opt/conda/lib/python3.10/site-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/opt/conda/lib/python3.10/site-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/opt/conda/lib/python3.10/site-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/opt/conda/lib/python3.10/site-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/opt/conda/lib/python3.10/site-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/opt/conda/lib/python3.10/site-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/opt/conda/lib/python3.10/site-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/opt/conda/lib/python3.10/site-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/opt/conda/lib/python3.10/site-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/opt/conda/lib/python3.10/site-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/opt/conda/lib/python3.10/site-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/opt/conda/lib/python3.10/site-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/opt/conda/lib/python3.10/site-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/opt/conda/lib/python3.10/site-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/opt/conda/lib/python3.10/site-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/opt/conda/lib/python3.10/site-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/opt/conda/lib/python3.10/site-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/opt/conda/lib/python3.10/site-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/opt/conda/lib/python3.10/site-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/opt/conda/lib/python3.10/site-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/opt/conda/lib/python3.10/site-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/opt/conda/lib/python3.10/site-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/opt/conda/lib/python3.10/site-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/opt/conda/lib/python3.10/site-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/opt/conda/lib/python3.10/site-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/opt/conda/lib/python3.10/site-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/opt/conda/lib/python3.10/site-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/opt/conda/lib/python3.10/site-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/opt/conda/lib/python3.10/site-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/opt/conda/lib/python3.10/site-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/opt/conda/lib/python3.10/site-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/opt/conda/lib/python3.10/site-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/opt/conda/lib/python3.10/site-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/opt/conda/lib/python3.10/site-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/opt/conda/lib/python3.10/site-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/opt/conda/lib/python3.10/site-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/opt/conda/lib/python3.10/site-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/opt/conda/lib/python3.10/site-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/opt/conda/lib/python3.10/site-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/opt/conda/lib/python3.10/site-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/opt/conda/lib/python3.10/site-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/opt/conda/lib/python3.10/site-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/opt/conda/lib/python3.10/site-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/opt/conda/lib/python3.10/site-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/opt/conda/lib/python3.10/site-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/opt/conda/lib/python3.10/site-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/opt/conda/lib/python3.10/site-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/opt/conda/lib/python3.10/site-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/opt/conda/lib/python3.10/site-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/opt/conda/lib/python3.10/site-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/opt/conda/lib/python3.10/site-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/opt/conda/lib/python3.10/site-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/opt/conda/lib/python3.10/site-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/opt/conda/lib/python3.10/site-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/opt/conda/lib/python3.10/site-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    }
   ],
   "source": [
    "scores = evaluate_scores(train_true, train_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9697847d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bleu: 0.00805811755004154\n",
      "bleu: 0.133922112029015\n",
      "bleu: 4.0216221822400775e-232\n",
      "bert_sim: 0.5676581859588623\n",
      "bert_sim: 0.7076610922813416\n",
      "bert_sim: 0.3131912648677826\n"
     ]
    }
   ],
   "source": [
    "for metric, values in scores.items():\n",
    "    print(f\"{metric}: {np.mean(values)}\")\n",
    "    print(f\"{metric}: {np.max(values)}\")\n",
    "    print(f\"{metric}: {np.min(values)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3d3911f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This is a live performance of a folk song. The tempo is fast with a groovy bass line, groovy drumming, groovy bass line, groovy bass line, groovy bass line, groovy bass line, groovy bass line, groovy bass line, groovy bass line, groovy bass'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inference(torch.randn(512,).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab6c97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[\"filenames\"][:5]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
