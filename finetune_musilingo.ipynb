{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe621ff-f7cd-4346-b72d-c872214abda3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import Wav2Vec2FeatureExtractor\n",
    "from transformers import StoppingCriteria, StoppingCriteriaList\n",
    "from transformers import AutoModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c29344a4-7eb1-48d2-b0cd-bfd58cb53f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "musilingo = AutoModel.from_pretrained(\"m-a-p/MusiLingo-short-v1\", trust_remote_code=True)\n",
    "musilingo.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ba26ef-d8fc-4458-9fab-64a613934b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StoppingCriteriaSub(StoppingCriteria):\n",
    "    def __init__(self, stops=[], encounters=1):\n",
    "        super().__init__()\n",
    "        self.stops = stops\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor):\n",
    "        for stop in self.stops:\n",
    "            if torch.all((stop == input_ids[0][-len(stop):])).item():\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "def get_musilingo_pred(model, text, audio_path, stopping, length_penalty=1, temperature=0.1,\n",
    "    max_new_tokens=300, num_beams=1, min_length=1, top_p=0.5, repetition_penalty=1.0):\n",
    "\n",
    "    # see https://huggingface.co/m-a-p/MusiLingo-musicqa-v1 for load_audio function definition\n",
    "    audio = load_audio(audio_path, target_sr=24000,\n",
    "                        is_mono=True,\n",
    "                        is_normalize=False,\n",
    "                        crop_to_length_in_sample_points=int(30*16000)+1,\n",
    "                        crop_randomly=True,\n",
    "                        pad=False).cuda()\n",
    "    processor = Wav2Vec2FeatureExtractor.from_pretrained(\"m-a-p/MERT-v1-330M\",trust_remote_code=True)\n",
    "    audio = processor(audio,\n",
    "                    sampling_rate=24000,\n",
    "                    return_tensors=\"pt\")['input_values'][0].cuda()\n",
    "\n",
    "    audio_embeds, atts_audio = model.encode_audio(audio)\n",
    "\n",
    "    prompt = '<Audio><AudioHere></Audio> ' + text\n",
    "    instruction_prompt = [model.prompt_template.format(prompt)]\n",
    "    audio_embeds, atts_audio = model.instruction_prompt_wrap(audio_embeds, atts_audio, instruction_prompt)\n",
    "\n",
    "    model.llama_tokenizer.padding_side = \"right\"\n",
    "    batch_size = audio_embeds.shape[0]\n",
    "    bos = torch.ones([batch_size, 1],\n",
    "                    dtype=torch.long,\n",
    "                    device=torch.device('cuda')) * model.llama_tokenizer.bos_token_id\n",
    "    bos_embeds = model.llama_model.model.embed_tokens(bos)\n",
    "    # atts_bos = atts_audio[:, :1]\n",
    "    inputs_embeds = torch.cat([bos_embeds, audio_embeds], dim=1)\n",
    "    # attention_mask = torch.cat([atts_bos, atts_audio], dim=1)\n",
    "    outputs = model.llama_model.generate(\n",
    "        inputs_embeds=inputs_embeds,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        stopping_criteria=stopping,\n",
    "        num_beams=num_beams,\n",
    "        do_sample=True,\n",
    "        min_length=min_length,\n",
    "        top_p=top_p,\n",
    "        repetition_penalty=repetition_penalty,\n",
    "        length_penalty=length_penalty,\n",
    "        temperature=temperature,\n",
    "    )\n",
    "    output_token = outputs[0]\n",
    "    if output_token[0] == 0:  # the model might output a unknow token <unk> at the beginning. remove it\n",
    "        output_token = output_token[1:]\n",
    "    if output_token[0] == 1:  # if there is a start token <s> at the beginning. remove it\n",
    "        output_token = output_token[1:]\n",
    "    output_text = model.llama_tokenizer.decode(output_token, add_special_tokens=False)\n",
    "    output_text = output_text.split('###')[0]  # remove the stop sign '###'\n",
    "    output_text = output_text.split('Assistant:')[-1].strip()\n",
    "    return output_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48fee65-8267-4172-b6af-93dd2e821c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_audio(\n",
    "    file_path,\n",
    "    target_sr,\n",
    "    is_mono=True,\n",
    "    is_normalize=False,\n",
    "    crop_to_length_in_sec=None,\n",
    "    crop_to_length_in_sample_points=None,\n",
    "    crop_randomly=False,\n",
    "    pad=False,\n",
    "    return_start=False,\n",
    "    device=torch.device('cpu')\n",
    "):\n",
    "    \"\"\"Load audio file and convert to target sample rate.\n",
    "    Supports cropping and padding.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): path to audio file\n",
    "        target_sr (int): target sample rate, if not equal to sample rate of audio file, resample to target_sr\n",
    "        is_mono (bool, optional): convert to mono. Defaults to True.\n",
    "        is_normalize (bool, optional): normalize to [-1, 1]. Defaults to False.\n",
    "        crop_to_length_in_sec (float, optional): crop to specified length in seconds. Defaults to None.\n",
    "        crop_to_length_in_sample_points (int, optional): crop to specified length in sample points. Defaults to None. Note that the crop length in sample points is calculated before resampling.\n",
    "        crop_randomly (bool, optional): crop randomly. Defaults to False.\n",
    "        pad (bool, optional): pad to specified length if waveform is shorter than specified length. Defaults to False.\n",
    "        device (torch.device, optional): device to use for resampling. Defaults to torch.device('cpu').\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: waveform of shape (1, n_sample)\n",
    "    \"\"\"\n",
    "    # TODO: deal with target_depth\n",
    "    try:\n",
    "        waveform, sample_rate = torchaudio.load(file_path)\n",
    "    except Exception as e:\n",
    "        waveform, sample_rate = torchaudio.backend.soundfile_backend.load(file_path)\n",
    "    if waveform.shape[0] > 1:\n",
    "        if is_mono:\n",
    "            waveform = torch.mean(waveform, dim=0, keepdim=True)\n",
    "\n",
    "    if is_normalize:\n",
    "        waveform = waveform / waveform.abs().max()\n",
    "\n",
    "    waveform, start = crop_audio(\n",
    "        waveform,\n",
    "        sample_rate,\n",
    "        crop_to_length_in_sec=crop_to_length_in_sec,\n",
    "        crop_to_length_in_sample_points=crop_to_length_in_sample_points,\n",
    "        crop_randomly=crop_randomly,\n",
    "        pad=pad,\n",
    "    )\n",
    "\n",
    "    if sample_rate != target_sr:\n",
    "        resampler = torchaudio.transforms.Resample(sample_rate, target_sr)\n",
    "        waveform = waveform.to(device)\n",
    "        resampler = resampler.to(device)\n",
    "        waveform = resampler(waveform)\n",
    "\n",
    "    if return_start:\n",
    "        return waveform, start\n",
    "    return waveform\n",
    "\n",
    "def crop_audio(\n",
    "    waveform,\n",
    "    sample_rate,\n",
    "    crop_to_length_in_sec=None,\n",
    "    crop_to_length_in_sample_points=None,\n",
    "    crop_randomly=False,\n",
    "    pad=False,\n",
    "):\n",
    "    \"\"\"Crop waveform to specified length in seconds or sample points.\n",
    "    Supports random cropping and padding.\n",
    "\n",
    "    Args:\n",
    "        waveform (torch.Tensor): waveform of shape (1, n_sample)\n",
    "        sample_rate (int): sample rate of waveform\n",
    "        crop_to_length_in_sec (float, optional): crop to specified length in seconds. Defaults to None.\n",
    "        crop_to_length_in_sample_points (int, optional): crop to specified length in sample points. Defaults to None.\n",
    "        crop_randomly (bool, optional): crop randomly. Defaults to False.\n",
    "        pad (bool, optional): pad to specified length if waveform is shorter than specified length. Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: cropped waveform\n",
    "        int: start index of cropped waveform in original waveform\n",
    "    \"\"\"\n",
    "    assert crop_to_length_in_sec is None or crop_to_length_in_sample_points is None, \\\n",
    "    \"Only one of crop_to_length_in_sec and crop_to_length_in_sample_points can be specified\"\n",
    "\n",
    "    # convert crop length to sample points\n",
    "    crop_duration_in_sample = None\n",
    "    if crop_to_length_in_sec:\n",
    "        crop_duration_in_sample = int(sample_rate * crop_to_length_in_sec)\n",
    "    elif crop_to_length_in_sample_points:\n",
    "        crop_duration_in_sample = crop_to_length_in_sample_points\n",
    "\n",
    "    # crop\n",
    "    start = 0\n",
    "    if crop_duration_in_sample:\n",
    "        if waveform.shape[-1] > crop_duration_in_sample:\n",
    "            if crop_randomly:\n",
    "                start = random.randint(0, waveform.shape[-1] - crop_duration_in_sample)\n",
    "            waveform = waveform[..., start:start + crop_duration_in_sample]\n",
    "\n",
    "        elif waveform.shape[-1] < crop_duration_in_sample:\n",
    "            if pad:\n",
    "                waveform = torch.nn.functional.pad(waveform, (0, crop_duration_in_sample - waveform.shape[-1]))\n",
    "\n",
    "    return waveform, start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d9157e-2ebc-4877-8339-0870ddbc2de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchaudio\n",
    "prompt = \"describe this song\"\n",
    "audio_path = \"/content/_8OIugVSFeE.wav\"\n",
    "stopping = StoppingCriteriaList([StoppingCriteriaSub([torch.tensor([835]).cuda(),\n",
    "                                  torch.tensor([2277, 29937]).cuda()])])\n",
    "response = get_musilingo_pred(musilingo.model, prompt, audio_path, stopping, length_penalty=100, temperature=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "51e6a4ef-37c0-44ed-a315-1b767a21b53a",
   "metadata": {},
   "outputs": [],
   "source": [
    "musilingo = AutoModel.from_pretrained(\"m-a-p/MusiLingo-short-v1\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "33573192-edbc-4a54-8a8c-3a9b4c93b9e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MusilingoModel(\n",
       "  (model): MusiLingo(\n",
       "    (audio_encoder): MERTModel(\n",
       "      (feature_extractor): HubertFeatureEncoder(\n",
       "        (conv_layers): ModuleList(\n",
       "          (0): HubertGroupNormConvLayer(\n",
       "            (conv): Conv1d(1, 512, kernel_size=(10,), stride=(5,), bias=False)\n",
       "            (activation): GELUActivation()\n",
       "            (layer_norm): GroupNorm(512, 512, eps=1e-05, affine=True)\n",
       "          )\n",
       "          (1-4): 4 x HubertNoLayerNormConvLayer(\n",
       "            (conv): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (5-6): 2 x HubertNoLayerNormConvLayer(\n",
       "            (conv): Conv1d(512, 512, kernel_size=(2,), stride=(2,), bias=False)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (feature_projection): MERTFeatureProjection(\n",
       "        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (projection): Linear(in_features=512, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (encoder): HubertEncoderStableLayerNorm(\n",
       "        (pos_conv_embed): HubertPositionalConvEmbedding(\n",
       "          (conv): ParametrizedConv1d(\n",
       "            1024, 1024, kernel_size=(128,), stride=(1,), padding=(64,), groups=16\n",
       "            (parametrizations): ModuleDict(\n",
       "              (weight): ParametrizationList(\n",
       "                (0): _WeightNorm()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (padding): HubertSamePadLayer()\n",
       "          (activation): GELUActivation()\n",
       "        )\n",
       "        (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (layers): ModuleList(\n",
       "          (0-23): 24 x HubertEncoderLayerStableLayerNorm(\n",
       "            (attention): HubertSdpaAttention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (feed_forward): HubertFeedForward(\n",
       "              (intermediate_dropout): Dropout(p=0.0, inplace=False)\n",
       "              (intermediate_dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "              (output_dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (output_dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (llama_model): LlamaForCausalLM(\n",
       "      (model): LlamaModel(\n",
       "        (embed_tokens): Embedding(32001, 4096, padding_idx=0)\n",
       "        (layers): ModuleList(\n",
       "          (0-31): 32 x LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "              (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "              (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "              (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "              (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "              (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm()\n",
       "            (post_attention_layernorm): LlamaRMSNorm()\n",
       "          )\n",
       "        )\n",
       "        (norm): LlamaRMSNorm()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=4096, out_features=32001, bias=False)\n",
       "    )\n",
       "    (llama_proj): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "musilingo.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d584af28-81b4-4168-af15-9f744cd125d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/cs229/lib/python3.10/site-packages/timm/models/hub.py:4: FutureWarning: Importing from timm.models.hub is deprecated, please import via timm.models\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.models\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0aa95f0b1daf4c1994a02db0d41c9234",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15fb8eec6b404df9a3a303fddbafade1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00003.safetensors:  23%|##3       | 1.15G/4.95G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58d3dcb7afe34391a87d902c982e7968",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00003.safetensors:   0%|          | 0.00/4.82G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Audio Encoder\n",
      "WARNING: feature_extractor_cqt requires the libray 'nnAudio'\n",
      "Loading Audio Encoder Done\n",
      "Loading LLAMA\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34a86a15e7cf4bcd898115b0f7e6556f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/727 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fee1c98474be4221bdbd4d583478793a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9539266f6344458eaa0967cfc858137d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/411 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a71bfec786945deb69985a44d6bef60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/619 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e2b0c1bb93f4544977b22879a0d5d60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin.index.json:   0%|          | 0.00/26.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c209022de9114805b17d22f7308f2fc6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "018ec6049cbe418f8b26166ac13f5da4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00001-of-00002.bin:   0%|          | 0.00/9.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "692b3150a92e48a5be3e70729ed5c46e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00002-of-00002.bin:   0%|          | 0.00/3.50G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LlamaForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "750327c359264b2abf6e94806268cc86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3102a9005cb94958b4fbfed2f7e28d17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/137 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading LLAMA Done\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34d9a863cfe942c69c503ba691904c16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "AttributeError",
     "evalue": "'MusilingoModel' object has no attribute 'llama_tokenizer'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 52\u001b[0m\n\u001b[1;32m     49\u001b[0m musilingo \u001b[38;5;241m=\u001b[39m AutoModel\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mm-a-p/MusiLingo-short-v1\u001b[39m\u001b[38;5;124m\"\u001b[39m, trust_remote_code\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     51\u001b[0m \u001b[38;5;66;03m# Dataset\u001b[39;00m\n\u001b[0;32m---> 52\u001b[0m dataset \u001b[38;5;241m=\u001b[39m AudioTextDataset(\u001b[38;5;28mlist\u001b[39m(train_data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mytid\u001b[39m\u001b[38;5;124m\"\u001b[39m]), \u001b[38;5;28mlist\u001b[39m(train_data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcaption\u001b[39m\u001b[38;5;124m\"\u001b[39m]), \u001b[43mmusilingo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllama_tokenizer\u001b[49m)\n\u001b[1;32m     54\u001b[0m \u001b[38;5;66;03m# DataLoader\u001b[39;00m\n\u001b[1;32m     55\u001b[0m dataloader \u001b[38;5;241m=\u001b[39m DataLoader(dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/opt/conda/envs/cs229/lib/python3.10/site-packages/torch/nn/modules/module.py:1931\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1929\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1930\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1931\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[1;32m   1932\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1933\u001b[0m )\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'MusilingoModel' object has no attribute 'llama_tokenizer'"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "from transformers import AutoModel\n",
    "\n",
    "to_exclude = [\"lwdDm3UO5WM\", \"sETUDPPoDuo\", \"W58kioYp1Ms\"]\n",
    "data_path = \"data/wav_files/wav-48\"\n",
    "train_data = pd.read_csv(\"train_labels.csv\")[[\"ytid\", \"caption\"]]\n",
    "train_data = train_data[~train_data['ytid'].isin(to_exclude)]\n",
    "train_data[\"ytid\"] = [f\"{data_path}/{filename}.wav\" for filename in train_data[\"ytid\"]]\n",
    "\n",
    "test_data = pd.read_csv(\"test_labels.csv\")[[\"ytid\", \"caption\"]]\n",
    "test_data = test_data[~test_data['ytid'].isin(to_exclude)]\n",
    "test_data[\"ytid\"] = [f\"{data_path}/{filename}.wav\" for filename in test_data[\"ytid\"]]\n",
    "\n",
    "class AudioTextDataset(Dataset):\n",
    "    def __init__(self, audio_paths, targets, processor):\n",
    "        self.audio_paths = audio_paths\n",
    "        self.targets = targets\n",
    "        self.processor = processor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.audio_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        waveform, sample_rate = torchaudio.load(self.audio_paths[idx])\n",
    "    \n",
    "        if waveform.size(0) > 1:\n",
    "            waveform = torch.mean(waveform, dim=0, keepdim=True)\n",
    "\n",
    "        if sample_rate != self.resample_rate:\n",
    "            resampler = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=24000)\n",
    "            waveform = resampler(waveform)\n",
    "            \n",
    "        waveform = torch.nn.functional.pad(waveform, (0, 240000 - len(waveform[0])), mode=\"constant\", value=0)\n",
    "        \n",
    "        audio_input = self.processor(waveform.squeeze().numpy(), sampling_rate=24000, return_tensors=\"pt\")\n",
    "\n",
    "        \n",
    "        audio = self.processor(audio, \n",
    "                        sampling_rate=24000, \n",
    "                        return_tensors=\"pt\")['input_values'][0].cuda() \n",
    "            \n",
    "        audio_embeds, atts_audio = model.encode_audio(audio)\n",
    "            \n",
    "        prompt = '<Audio><AudioHere></Audio> ' + text\n",
    "        instruction_prompt = [model.prompt_template.format(prompt)]\n",
    "        audio_embeds, atts_audio = model.instruction_prompt_wrap(audio_embeds, atts_audio, instruction_prompt)\n",
    "        \n",
    "        model.llama_tokenizer.padding_side = \"right\"\n",
    "        batch_size = audio_embeds.shape[0]\n",
    "        bos = torch.ones([batch_size, 1],\n",
    "                        dtype=torch.long,\n",
    "                        device=torch.device('cuda')) * model.llama_tokenizer.bos_token_id\n",
    "        bos_embeds = model.llama_model.model.embed_tokens(bos)\n",
    "        # atts_bos = atts_audio[:, :1]\n",
    "        inputs_embeds = torch.cat([bos_embeds, audio_embeds], dim=1)\n",
    "        \n",
    "        prompt = self.prompts[idx]\n",
    "        target = self.targets[idx]\n",
    "        \n",
    "        # Tokenize text\n",
    "        tokenized_prompt = self.tokenizer(\"describe this song\", return_tensors=\"pt\", padding=True)\n",
    "        tokenized_target = self.tokenizer(target, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "        return audio, tokenized_prompt, tokenized_target\n",
    "\n",
    "\n",
    "# Dataset\n",
    "dataset = AudioTextDataset(list(train_data[\"ytid\"]), list(train_data[\"caption\"]), musilingo.llama_tokenizer)\n",
    "\n",
    "# DataLoader\n",
    "dataloader = DataLoader(dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "# Training Setup\n",
    "optimizer = optim.AdamW(musilingo.parameters(), lr=5e-5)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# Training Loop\n",
    "for epoch in range(3):  # Number of epochs\n",
    "    musilingo.train()\n",
    "    for batch in dataloader:\n",
    "        audio, tokenized_prompt, tokenized_target = batch\n",
    "\n",
    "        # Forward pass\n",
    "        audio_embeds, atts_audio = musilingo.model.encode_audio(audio)\n",
    "        inputs_embeds = musilingo.model.instruction_prompt_wrap(audio_embeds, atts_audio, tokenized_prompt)\n",
    "\n",
    "        # Generate predictions\n",
    "        outputs = musilingo.model.llama_model(\n",
    "            inputs_embeds=inputs_embeds[\"inputs_embeds\"],\n",
    "            attention_mask=inputs_embeds[\"attention_mask\"],\n",
    "            labels=tokenized_target[\"input_ids\"]\n",
    "        )\n",
    "\n",
    "        # Compute loss\n",
    "        loss = outputs.loss\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch+1} - Loss: {loss.item()}\")\n",
    "    torch.save(model.state_dict(), f\"musilingo_weights_{epoch}.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0606d33b-65dc-4c01-b1f0-eefbae4b4a8a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
