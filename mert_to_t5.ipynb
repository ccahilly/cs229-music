{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "826f035a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoModel, Wav2Vec2FeatureExtractor, T5Tokenizer, T5ForConditionalGeneration\n",
    "import torchaudio.transforms as T\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Check for GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "\n",
    "# Load Pre-trained Models\n",
    "mert = AutoModel.from_pretrained(\"m-a-p/MERT-v1-330M\", trust_remote_code=True)\n",
    "processor = Wav2Vec2FeatureExtractor.from_pretrained(\"m-a-p/MERT-v1-330M\", trust_remote_code=True)\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
    "decoder = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
    "\n",
    "# Freeze MERT encoder\n",
    "for param in mert.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Aggregator for MERT outputs (weighted average over layers)\n",
    "aggregator = nn.Conv1d(in_channels=25, out_channels=1, kernel_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "aa2ac67b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchaudio\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class MusicDataset(Dataset):\n",
    "    def __init__(self, file_paths, descriptions, processor, resample_rate):\n",
    "        self.file_paths = file_paths\n",
    "        self.descriptions = descriptions\n",
    "        self.processor = processor\n",
    "        self.resample_rate = resample_rate\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_path = self.file_paths[idx]\n",
    "\n",
    "        waveform, sample_rate = torchaudio.load(file_path)\n",
    "    \n",
    "        if waveform.size(0) > 1:\n",
    "            waveform = torch.mean(waveform, dim=0, keepdim=True)\n",
    "\n",
    "        # print(\"Initial shape\")\n",
    "        # print(waveform.shape)\n",
    "        # print(sample_rate)\n",
    "\n",
    "        if sample_rate != self.resample_rate:\n",
    "            resampler = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=self.resample_rate)\n",
    "            waveform = resampler(waveform)\n",
    "        # print(self.resample_rate)\n",
    "        waveform = torch.nn.functional.pad(waveform, (0, 240000 - len(waveform[0])), mode=\"constant\", value=0)\n",
    "        # print(waveform.shape)\n",
    "\n",
    "        audio_input = self.processor(waveform.squeeze().numpy(), sampling_rate=self.resample_rate, return_tensors=\"pt\")\n",
    "\n",
    "        text = self.descriptions[idx]\n",
    "        # print(audio_input[\"input_values\"].shape)\n",
    "\n",
    "        return {\"audio_input\": audio_input, \"text\": text}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2400dfa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class AudioToTextModel(nn.Module):\n",
    "    def __init__(self, encoder, decoder, aggregator):\n",
    "        super(AudioToTextModel, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.aggregator = aggregator\n",
    "        self.projection = nn.Linear(1024, 512)  # 1024 -> 512\n",
    "\n",
    "    def forward(self, audio_input, decoder_input_ids=None, labels=None):\n",
    "#         print(audio_input[\"input_values\"].size())\n",
    "        with torch.no_grad():\n",
    "            outputs = self.encoder(**audio_input, output_hidden_states=True)\n",
    "#             print(\"Last output hidden\")\n",
    "#             print(outputs.last_hidden_state.shape)\n",
    "            final_layer_hidden = outputs.last_hidden_state\n",
    "        \n",
    "#         print(\"After encoder\")\n",
    "#         print(final_layer_hidden.shape)\n",
    "        time_reduced_hidden = final_layer_hidden[:, :512, :]\n",
    "#         print(\"Reduced time\")\n",
    "#         print(time_reduced_hidden.shape)\n",
    "        \n",
    "        projected_embeddings = self.projection(time_reduced_hidden) \n",
    "#         print(\"After projection\")\n",
    "#         print(projected_embeddings.size())\n",
    "        \n",
    "#         seq_length = decoder_input_ids.size(1)\n",
    "#         projected_embeddings = projected_embeddings.unsqueeze(1) #.expand(-1, seq_length, -1)\n",
    "        # print(\"Expanded projected embeddings\")\n",
    "        # print(projected_embeddings.size())\n",
    "        \n",
    "        # print(\"Decoder input ids size\")\n",
    "        # print(decoder_input_ids.size())\n",
    "        \n",
    "        # print(labels.size())\n",
    "#         print(projected_embeddings.shape)\n",
    "        decoder_outputs = self.decoder(\n",
    "            inputs_embeds=projected_embeddings,\n",
    "            decoder_input_ids=decoder_input_ids,\n",
    "            labels=labels,\n",
    "        )\n",
    "        return decoder_outputs\n",
    "        \n",
    "    def generate(self, audio_input, max_length=50):\n",
    "        \"\"\"\n",
    "        Generates text from audio input using the decoder's generate method.\n",
    "        \n",
    "        Args:\n",
    "            audio_input: Preprocessed audio input.\n",
    "            max_length: Maximum length of the output sequence.\n",
    "            num_beams: Number of beams for beam search (default: 1 for greedy decoding).\n",
    "\n",
    "        Returns:\n",
    "            Generated token IDs.\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            outputs = self.encoder(**audio_input, output_hidden_states=True)\n",
    "#             print(\"Last output hidden\")\n",
    "#             print(outputs.last_hidden_state.shape)\n",
    "            final_layer_hidden = outputs.last_hidden_state\n",
    "        \n",
    "#         print(\"After encoder\")\n",
    "#         print(final_layer_hidden.shape)\n",
    "        time_reduced_hidden = final_layer_hidden[:, :512, :]\n",
    "#         print(\"Reduced time\")\n",
    "#         print(time_reduced_hidden.shape)\n",
    "        \n",
    "        projected_embeddings = self.projection(time_reduced_hidden)\n",
    "        print(\"Projected embeddings shape\")\n",
    "        print(projected_embeddings.shape)\n",
    "        \n",
    "        print(projected_embeddings)\n",
    "\n",
    "        # Use the decoder's `generate` method\n",
    "        generated_ids = self.decoder.generate(\n",
    "            inputs_embeds=projected_embeddings,\n",
    "            max_length=max_length,\n",
    "        )\n",
    "        return generated_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "97b62269",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Model\n",
    "model = AudioToTextModel(encoder=mert, decoder=decoder, aggregator=aggregator).to(device)\n",
    "\n",
    "# Optimizer (only train decoder and aggregator)\n",
    "optimizer = torch.optim.AdamW([\n",
    "    {\"params\": model.decoder.parameters(), \"lr\": 5e-5},\n",
    "    {\"params\": model.aggregator.parameters(), \"lr\": 5e-5},\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "73d9d782",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    audio_inputs = {key: torch.cat([item[\"audio_input\"][key] for item in batch], dim=0) for key in batch[0][\"audio_input\"]}\n",
    "    captions = [item[\"text\"] for item in batch]\n",
    "\n",
    "    # Tokenize captions\n",
    "    tokenized = tokenizer(captions, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    decoder_input_ids = tokenized.attention_mask\n",
    "    labels = tokenized.input_ids\n",
    "    \n",
    "\n",
    "    return audio_inputs, decoder_input_ids, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3118c9be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "to_exclude = [\"lwdDm3UO5WM\", \"sETUDPPoDuo\", \"W58kioYp1Ms\"]\n",
    "data_path = \"data/wav_files/wav-48\"\n",
    "train_data = pd.read_csv(\"train_labels.csv\")[[\"ytid\", \"caption\"]]\n",
    "train_data = train_data[~train_data['ytid'].isin(to_exclude)]\n",
    "train_data[\"ytid\"] = [f\"{data_path}/{filename}.wav\" for filename in train_data[\"ytid\"]]\n",
    "\n",
    "test_data = pd.read_csv(\"test_labels.csv\")[[\"ytid\", \"caption\"]]\n",
    "test_data = test_data[~test_data['ytid'].isin(to_exclude)]\n",
    "test_data[\"ytid\"] = [f\"{data_path}/{filename}.wav\" for filename in test_data[\"ytid\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fe641604-0077-4ead-88f2-f45040b80cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"train_labels.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "11d3dd09-d819-44fe-8b2a-51002b27f5f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4061"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8a104f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "resample_rate = processor.sampling_rate\n",
    "dataset = MusicDataset(list(train_data[\"ytid\"]), list(train_data[\"caption\"]), processor, resample_rate)\n",
    "test_dataset = MusicDataset(list(test_data[\"ytid\"]), list(test_data[\"caption\"]), processor, resample_rate)\n",
    "dataloader = DataLoader(dataset, batch_size=8, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c1dd370a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 14.280972480773926\n",
      "Epoch: 0, Loss: 15.891775131225586\n",
      "Epoch: 0, Loss: 14.784863471984863\n",
      "Epoch: 0, Loss: 14.836258888244629\n",
      "Epoch: 0, Loss: 14.163176536560059\n",
      "Epoch: 0, Loss: 13.0753755569458\n",
      "Epoch: 0, Loss: 12.0661039352417\n",
      "Epoch: 0, Loss: 12.638932228088379\n",
      "Epoch: 0, Loss: 10.87246036529541\n",
      "Epoch: 0, Loss: 11.236712455749512\n",
      "Epoch: 0, Loss: 9.964585304260254\n",
      "Epoch: 0, Loss: 9.869051933288574\n",
      "Epoch: 0, Loss: 8.920085906982422\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[41], line 25\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Compute loss and backpropagate\u001b[39;00m\n\u001b[1;32m     24\u001b[0m loss \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mloss\n\u001b[0;32m---> 25\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     27\u001b[0m losses\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mitem())\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# Training Loop\n",
    "num_epochs = 3\n",
    "losses = []\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in dataloader:\n",
    "        \n",
    "        # Move inputs and labels to GPU\n",
    "        audio_inputs = {key: value.to(device) for key, value in batch[0].items()}\n",
    "        decoder_input_ids = batch[1].to(device)\n",
    "        labels = batch[2].to(device)\n",
    "        \n",
    "        # print(\"Audio input device:\", {key: val.device for key, val in audio_inputs.items()})\n",
    "        # print(\"Decoder input IDs device:\", decoder_input_ids.device)\n",
    "        # print(\"Labels device:\", labels.device)\n",
    "        # print(\"Model device:\", next(model.parameters()).device)\n",
    "\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(audio_inputs, decoder_input_ids=decoder_input_ids, labels=labels)\n",
    "\n",
    "        # Compute loss and backpropagate\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        print(f\"Epoch: {epoch}, Loss: {loss.item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea6ae93",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52603607",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_audio(file_path, resample_rate=24000, max_length=240000):\n",
    "    \"\"\"Preprocesses the audio file: loads, converts to mono, resamples, and pads/trims.\"\"\"\n",
    "    waveform, sample_rate = torchaudio.load(file_path)\n",
    "        \n",
    "    if waveform.size(0) > 1:\n",
    "        waveform = torch.mean(waveform, dim=0, keepdim=True)\n",
    "\n",
    "    # print(\"Initial shape\")\n",
    "    # print(waveform.shape)\n",
    "    # print(sample_rate)\n",
    "\n",
    "    if sample_rate != 24000:\n",
    "        resampler = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=24000)\n",
    "        waveform = resampler(waveform)\n",
    "    waveform = torch.nn.functional.pad(waveform, (0, 240000 - len(waveform[0])), mode=\"constant\", value=0)\n",
    "    audio_input = processor(waveform.squeeze().numpy(), sampling_rate=24000, return_tensors=\"pt\")\n",
    "    return audio_input\n",
    "\n",
    "def inference(audio_file_path):\n",
    "    \"\"\"Takes an audio file path, preprocesses it, and outputs the generated text.\"\"\"\n",
    "    # Preprocess the audio\n",
    "    audio_input = preprocess_audio(audio_file_path).to(device)\n",
    "    print(audio_input)\n",
    "#     print(model(audio_input))\n",
    "\n",
    "    # Generate text\n",
    "#     model.decoder.eval()\n",
    "#     print(model(audio_input))\n",
    "    \n",
    "    generated_tokens = model.generate(audio_input)  # Adjust max_length as needed\n",
    "    print(generated_tokens)\n",
    "\n",
    "    # Decode the generated tokens to text\n",
    "    predicted_text = tokenizer.decode(generated_tokens[0], skip_special_tokens=True)\n",
    "    \n",
    "    return predicted_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42efc573-5b10-42ff-8330-6af9864f5130",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 11\n",
    "print(train_data[\"ytid\"][index])\n",
    "res = inference(train_data[\"ytid\"][index])\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dca7cb4-86bb-4a6f-b60b-48153b06c990",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder2 = T5ForConditionalGeneration.from_pretrained(\"t5-small\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39604e28-8e29-4206-9378-d263b3fad669",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_input = torch.randn(1, 512, 512).to(device)  # Simulated embeddings\n",
    "out_tokens = model.decoder.generate(inputs_embeds=sample_input)\n",
    "out_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e2debf3-ad78-4402-9ddc-54722536e35c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(out_tokens[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a79a38-d4cf-4de9-bdbe-05f25b6ab562",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"mert_t5_a2t.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9117f1a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
